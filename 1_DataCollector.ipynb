{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Data Collector\n",
    "\n",
    "This notebook will be used to collect data to be used in the rest of the dashboard. Each\n",
    "cell will be a self-contained codebase to collect data for a single data point, and will\n",
    "correspond to a similar cell within The Dashboard UI notebook. Data persistence for these\n",
    "notebooks will be in locally-stored CSV files (that can be changed easily by updating a\n",
    "shared data persistence function), and the general execution flow will be as follows:\n",
    "\n",
    "## Check last import date\n",
    "Each data type and source will have a hard-coded Data Import Frequency variable, set by\n",
    "looking at historical update frequency for said data. Checking this against the last import\n",
    "date of the data in storage protects against unnecessary data pulls, network IO, and IP\n",
    "blocking from data sources.\n",
    "\n",
    "## Import data\n",
    "If our cell passes the last import date gate, then we append new data to our existing source\n",
    "in persistent storage.\n",
    "\n",
    "## Check for consistency\n",
    "Once data is imported, new data is checked against existing data to compare for consistency,\n",
    "outliers, and missing values. If inconsistencies or missing values are found, the import is\n",
    "flagged for human review.\n",
    "\n",
    "# Shared functions\n",
    "This cell contains functions to be used among all collectors, to minimize duplicated code.\n",
    "*Note*: this cell _must_ be initialized before running any collector cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# function that checks if file in persistent storage exists\n",
    "def file_exists(collector_type: str, collector_subtype: str, file_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Takes in a collector type, collector subtype, and file name and returns True if file exists and\n",
    "    False otherwise.\n",
    "    :param collector_type:\n",
    "    :param collector_subtype:\n",
    "    :param file_name:\n",
    "    :return: boolean True if file exists and False otherwise\n",
    "    \"\"\"\n",
    "    target_directory = os.path.join('data', 'output', collector_type, collector_subtype)\n",
    "    target_file = os.path.join(target_directory, file_name)\n",
    "\n",
    "    if os.path.isfile(target_file):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# function that opens or creates files in persistent storage\n",
    "def file_opener(collector_type: str, collector_subtype: str, file_name: str) -> io:\n",
    "    \"\"\"\n",
    "    Takes in a collector type, collector subtype, and file name and either opens the file if it exists\n",
    "    :param collector_type:\n",
    "    :param collector_subtype:\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    target_directory = os.path.join('data', 'output', collector_type, collector_subtype)\n",
    "    target_file = os.path.join(target_directory, file_name)\n",
    "\n",
    "    if os.path.isfile(target_file):\n",
    "        f = open(target_file, 'a')\n",
    "        print('directory and file exist: file open for append')\n",
    "        return f\n",
    "    elif os.path.isdir(target_directory):\n",
    "        # f = open(target_file, 'x')\n",
    "        # print('directory exists: file created for write')\n",
    "\n",
    "        print('directory exists: ready for file write')\n",
    "        return None\n",
    "    else:\n",
    "        os.makedirs(target_directory)\n",
    "        # f = open(target_file, 'x')\n",
    "        print('neither directory nor file exist: ready for file write')\n",
    "        return None\n",
    "\n",
    "\n",
    "def file_closer(file) -> bool:\n",
    "    file.close()\n",
    "    print(\"file IO has terminated.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# function that checks CSV date last updated and compares it to today\n",
    "def file_last_updated(file_name, update_frequency: int) -> bool:\n",
    "    \"\"\"\n",
    "    Checks when the file in persistent storage was last updated, compares it to the run date,\n",
    "    then compares the time delta to the update frequency scheduled.\n",
    "\n",
    "    :param file_name:\n",
    "    :param update_frequency: how often the data needs to be refreshed, in days\n",
    "    :return: boolean indicating whether or not the data needs to be updated in this run\n",
    "    \"\"\"\n",
    "    file_path_representation = file_name.name\n",
    "    stats = os.stat(file_path_representation)\n",
    "\n",
    "    date_last_updated = datetime.datetime.fromtimestamp(stats.st_mtime)\n",
    "    current_datetime = datetime.datetime.today()\n",
    "    date_update_required = date_last_updated + datetime.timedelta(days=update_frequency)\n",
    "\n",
    "    print('file last updated:\\t\\t' + str(date_last_updated))\n",
    "    print('current datetime:\\t\\t' + str(current_datetime))\n",
    "    print('next update required on:\\t' + str(date_update_required))\n",
    "    print('\\tin ' + str(date_update_required - current_datetime) + ' and counting...')\n",
    "\n",
    "    if date_update_required <= current_datetime:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# stub function that connects to an API to collect data based on CSV last updated date\n",
    "def API_downloader(url: str, params: dict) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "# stub function that scrapes website (if no API available) based on CSV last updated date\n",
    "def scraper_downloader(url: str, params: dict) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "def file_downloader(url: str, collector_type: str, collector_subtype: str, file_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Downloads a single file from any given URL.\n",
    "    :param url: (string) a URL pointing to a file to be downloaded\n",
    "    :param collector_type:\n",
    "    :param collector_subtype:\n",
    "    :param file_name:\n",
    "\n",
    "    :return: return code (0 for success, 1 for failure)\n",
    "    \"\"\"\n",
    "    target_directory = os.path.join('data', 'output', collector_type, collector_subtype)\n",
    "    target_file = os.path.join(target_directory, file_name)\n",
    "\n",
    "    data = requests.get(url)\n",
    "\n",
    "    with open(target_file, 'wb') as f:\n",
    "        f.write(data.content)\n",
    "        print('data written to file\\n')\n",
    "        status_code = 0\n",
    "\n",
    "    if file_name.endswith('zip'):\n",
    "        with zipfile.ZipFile(target_file, 'r') as zip_ref:\n",
    "            try:\n",
    "                zip_ref.extractall(target_file[:-4])\n",
    "                print('file unzipped\\n')\n",
    "            except:\n",
    "                print('error in unzipping file')\n",
    "                status_code = 1\n",
    "                return status_code\n",
    "        # removes the original zip file to save space\n",
    "        os.remove(target_file)\n",
    "        # creates a placeholder file with the same name in order to use file_exists and filefile_last_updated functions\n",
    "        with open(target_file, 'w'):\n",
    "            pass\n",
    "\n",
    "    return status_code\n",
    "\n",
    "\n",
    "def file_extractor(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Extracts files from a compressed file format, then deletes the original compressed file.\n",
    "\n",
    "    :return: return code (0 for success, 1 for failure)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def file_writer(f: io, dataframe: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Takes in a file object and a pandas DataFrame, and writes the DataFrame to the file object\n",
    "    location.\n",
    "\n",
    "    Note: the DataFrame MUST be fully prepared for the file write, since this function does\n",
    "    no processing on the DataFrame.\n",
    "\n",
    "    :param f: a file object\n",
    "    :param dataframe: a pandas DataFrame\n",
    "    :return: status code - 0 if successful, 1 if failed\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(f, sep='\\t')\n",
    "\n",
    "    status_code = 0\n",
    "    return status_code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.1 Geographic Data: Basemaps\n",
    "This consists of international, supranational, national, and province-level boundaries, as\n",
    "well as major cities. Data is meant to be used as base layers for other geospatial products.\n",
    "\n",
    "Data Source: The World Bank\n",
    "Dataset Name: World Bank Official Boundaries\n",
    "Update Frequency: yearly (365 days)\n",
    "Collector Type: Geographic Data\n",
    "Collector Sub-Type: Basemaps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory and file exist: file open for append\n",
      "file last updated:\t\t2020-09-02 17:37:59.743737\n",
      "current datetime:\t\t2020-09-02 17:38:10.050210\n",
      "next update required on:\t2021-09-02 17:37:59.743737\n",
      "\tin 364 days, 23:59:49.693527 and counting...\n",
      "update deferred\n",
      "file IO has terminated.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def geographic_data__basemaps() -> int:\n",
    "    update_frequency = 365\n",
    "    update_required = False\n",
    "\n",
    "    type = 'geographic_data'\n",
    "    subtype = 'basemaps'\n",
    "    file_name = 'wb_boundaries_geojson_highres.zip'\n",
    "\n",
    "    saved_file_exists = file_exists(type, subtype, file_name)\n",
    "\n",
    "    # calls the file openers from the core functions\n",
    "    if saved_file_exists:\n",
    "        global_basemap_file = file_opener(type, subtype, file_name)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    source_urls = {'World Boundaries GeoJSON - Very High Resolution':\n",
    "                       'https://development-data-hub-s3-public.s3.amazonaws.com/ddhfiles/779551/wb_boundaries_geojson_highres.zip'}\n",
    "\n",
    "    # checks to see if data needs to be updated in this run\n",
    "    if saved_file_exists is False:\n",
    "        print('no file exists')\n",
    "        for name, url in source_urls.items():\n",
    "            file_downloader(url, type, subtype, file_name)\n",
    "    elif file_last_updated(global_basemap_file, update_frequency) or update_required:\n",
    "        print('file exists, updating...')\n",
    "        for name, url in source_urls:\n",
    "            file_downloader(url, type, subtype, file_name)\n",
    "    else:\n",
    "        print('update deferred')\n",
    "        # calls the file closer form the core functions\n",
    "        file_closer(global_basemap_file)\n",
    "        return 0\n",
    "\n",
    "geographic_data__basemaps()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}